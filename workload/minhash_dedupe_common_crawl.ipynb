{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3dd45a8",
   "metadata": {},
   "source": [
    "# UNFILTERED \n",
    "# Deduplicating Text in Common-Crawl for LLM Training. \n",
    "\n",
    "In this notebook, we will cover how to perform the minhash deduplication algorithm on html documents from the common crawl dataset.\n",
    "\n",
    "The Common Crawl corpus contains petabytes of data, with its oldest entries dating back to 2008, including raw web page data, metadata extracts, and text extracts.\n",
    "\n",
    "LLMs require massive amounts of data to train on. Early foundation models like GPT-3 and T5 saw improvements in model performance due to deduplication efforts. Deduplication makes it far less likely that the model regurgitates memorized text leading to better responses.\n",
    "\n",
    "*See [Deduplicating Training Data Makes Language Models Better (Lee et. all)](https://aclanthology.org/2022.acl-long.577.pdf)*\n",
    "\n",
    "---\n",
    "\n",
    "## The MinHash Deduplication algorithm\n",
    "\n",
    "If you google \"minhash deduplication\" you'll find a variety of sources that can walk you through the aglorithm. [Finding Near Duplicates with Jaccard Similarity and MinHash by Nelson Elhage](https://blog.nelhage.com/post/fuzzy-dedup/) is a great place to start, but if you are looking for the canonical reference for the MinHash deduplication algorithm, it originates from the seminal paper by Andrei Z. Broder, published in 1997, titled:\n",
    "\n",
    "```text\n",
    "\"On the resemblance and containment of documents\"\n",
    "Published in: Proceedings of the Compression and Complexity of Sequences 1997 (SEQUENCES '97)\n",
    "Publisher: IEEE Computer Society\n",
    "DOI: 10.1109/SEQUEN.1997.666900\n",
    "```\n",
    "\n",
    "A video walkthough of the algorithm is also available through [Mike Mull's presentation on YouTube](https://www.youtube.com/watch?v=KKNPmvELUP4). He even provides a [jupyter notebook](https://github.com/papers-we-love/san-diego/blob/master/presentations/2016-11-03-resemblance-containment-documents/Broder97.ipynb) detailing the core primatives and how they are calculated in pure python. \n",
    "\n",
    "In this notebook, we will adopt a more practical approach, leveraging daft primatives and small user-defined-functions to accelerate development and process documents at scale. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058dd5a5",
   "metadata": {},
   "source": [
    "### First we will need to authenticate with AWS to access S3\n",
    "\n",
    "Crawl data is free to access by anyone from anywhere. The data is hosted by Amazon Web Services’ Open Data Sets Sponsorships program on the bucket s3://commoncrawl/, located in the US-East-1 (Northern Virginia) AWS Region. The most performative means of accessing Common crawl is through s3, so you'll need to authenticate with an `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`. \n",
    "\n",
    "Common Crawl data can also be accessed without authentication, anonymously via it's http endpoint, but for the purposes of this walkthrough we are going to stick with S3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4bb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "from daft.io import IOConfig, S3Config\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Make sure to define your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in your environment variables or in a .env file\n",
    "load_dotenv()\n",
    "\n",
    "s3_config = S3Config(\n",
    "    region_name=\"us-east-1\",\n",
    "    requester_pays=True,\n",
    "    key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    anonymous=False,\n",
    ")\n",
    "\n",
    "IO_CONFIG = IOConfig(s3=s3_config)\n",
    "daft.set_planning_config(default_io_config=IO_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7920348",
   "metadata": {},
   "source": [
    "## Loading Common Crawl Documents \n",
    "\n",
    "We will be accessing Common Crawl through [WARC files](https://commoncrawl.org/blog/navigating-the-warc-file-format) since daft supports the format natively with `daft.read_warc(uri)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9348ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROWS = 1000 # We'll limit this demo to a small number of rows for our initial walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4980779",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_warc = daft.read_warc(\"s3://commoncrawl/crawl-data/CC-MAIN-2018-17/segments/*/warc/*.warc.gz\").limit(NUM_ROWS)\n",
    "df_warc.show(3) # Inspect the first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf049ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets investigate the different types of payloads we have: \n",
    "df_warc.select(\"WARC-Identified-Payload-Type\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95340f4b",
   "metadata": {},
   "source": [
    "### Step 1: Preprocessing\n",
    "Since we are primarily concerned with text, we will focus on text/html payloads, extracting text content from html body and normalizing the text itself. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from daft import col\n",
    "from daft.functions import monotonically_increasing_id\n",
    "\n",
    "\n",
    "# Define a UDF to remove http headers from the payload\n",
    "@daft.func()\n",
    "def remove_http_headers(x: str) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if len(x.split(\"\\r\\n\\r\\n\")) > 1:\n",
    "        return x.split(\"\\r\\n\\r\\n\")[1]\n",
    "    return \"\"\n",
    "\n",
    "# Filter the dataframe to only include text/html payloads\n",
    "df_html = df_warc.where(col(\"WARC-Identified-Payload-Type\")== \"text/html\")\n",
    "\n",
    "# Seperate the http headers from the payloads\n",
    "df_html = (\n",
    "    df_html\n",
    "    .with_column(\"content_raw\", remove_http_headers(col(\"warc_content\").try_decode(\"utf-8\")))\n",
    "    .where(col(\"content_raw\") != \"\")\n",
    ")   \n",
    "\n",
    "# Simplify the dataframe to just the content and add a monotonically increasing int id\n",
    "df_html = (\n",
    "    df_html\n",
    "    .with_columns_renamed({\"WARC-Identified-Payload-Type\": \"content_type\",})\n",
    "    .with_column(\"id\", monotonically_increasing_id())\n",
    "    .select(\"WARC-Record-ID\", \"id\", \"content_type\", \"content_raw\")\n",
    ")\n",
    "\n",
    "df_html.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab8f95",
   "metadata": {},
   "source": [
    "### Extracting Text from HTML body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selectolax.parser import HTMLParser\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "# Define a UDF to extract text from HTML content, Specifically (article, main, p, h1, h2, h3, li)\n",
    "@daft.func()\n",
    "def extract_blocks(html: str) -> list[str]:\n",
    "    tree = HTMLParser(html)\n",
    "    for n in tree.css(\"script,style,noscript\"):  n.decompose()\n",
    "    blocks = []\n",
    "    for node in tree.css(\"article, main, p, h1, h2, h3, li\"):\n",
    "        txt = node.text(separator=\" \", strip=True)\n",
    "        if not txt: \n",
    "            continue\n",
    "        blocks.append(txt)\n",
    "    return blocks\n",
    "\n",
    "df_text = df_html.with_column(\"content_text\", extract_blocks(col(\"content_raw\")).list.join(\" \"))\n",
    "df_text.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ccbe4a",
   "metadata": {},
   "source": [
    "# Minhash\n",
    "\n",
    "Now that we have extracted the text out of the html we move to normalize the inputs and calculate our minhash vectors using daft's `minhash` expression! No need to build shingles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d58fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 64 # Number of Permutations\n",
    "SEED = 43 # Seed for the hash function\n",
    "NGRAM_SIZE = 5 # Size of the n-grams\n",
    "index_col = \"id\" \n",
    "content_col = \"content_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8704a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize text \n",
    "df_norm = df_text.with_column(\"content_normalized\", \n",
    "    col(content_col).str.normalize(\n",
    "        remove_punct=True, \n",
    "        lowercase=True, \n",
    "        nfd_unicode=True, \n",
    "        white_space=True\n",
    "    ) \n",
    ")\n",
    "df_norm.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the minhash vectors\n",
    "df_minhash = (\n",
    "    df_norm\n",
    "    .with_column(\"min_hashes\", col(\"content_normalized\").minhash(\n",
    "        num_hashes = K,\n",
    "        ngram_size = NGRAM_SIZE,\n",
    "        seed = SEED, \n",
    "        hash_function = 'xxhash'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "df_minhash.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7760f",
   "metadata": {},
   "source": [
    "### Band Generation and Bucketing\n",
    "\n",
    "Next, we will:\n",
    "1. Use the optimal_param function to determine the best band (b) and row (r) parameters for our LSH bucketing\n",
    "2. Split each document's minhash vector into b bands of r rows each\n",
    "3. Create buckets by hashing each band's signature, grouping similar documents together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d43ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad as integrate\n",
    "\n",
    "def optimal_param(\n",
    "    threshold: float,\n",
    "    num_perm: int,\n",
    "    false_positive_weight: float = 0.5,\n",
    "    false_negative_weight: float = 0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the optimal `MinHashLSH` parameter that minimizes the weighted sum\n",
    "    of probabilities of false positive and false negative, taken from datasketch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold : float\n",
    "        The threshold for similarity.\n",
    "    num_perm : int\n",
    "        The number of permutations.\n",
    "    false_positive_weight : float\n",
    "        The weight of false positive.\n",
    "    false_negative_weight : float\n",
    "        The weight of false negative.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[int, int]\n",
    "        The optimal `b` and `r` parameters.\n",
    "        The number of bands, and the number of rows per band respectively.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> optimal_param(0.7, 256)\n",
    "    (25, 10)\n",
    "    \"\"\"\n",
    "\n",
    "    def false_positive_area(threshold: float, b: int, r: int):\n",
    "        \"\"\"Source: `datasketch.lsh`\"\"\"\n",
    "\n",
    "        def area(s):\n",
    "            return 1 - (1 - s ** float(r)) ** float(b)\n",
    "\n",
    "        a, _ = integrate(area, 0.0, threshold)\n",
    "        return a\n",
    "\n",
    "    def false_negative_area(threshold: float, b: int, r: int):\n",
    "        \"\"\"Source: `datasketch.lsh`\"\"\"\n",
    "\n",
    "        def area(s):\n",
    "            return 1 - (1 - (1 - s ** float(r)) ** float(b))\n",
    "\n",
    "        a, _ = integrate(area, threshold, 1.0)\n",
    "        return a\n",
    "\n",
    "    min_error = float(\"inf\")\n",
    "    opt = (0, 0)\n",
    "    for b in range(1, num_perm + 1):\n",
    "        max_r = int(num_perm / b)\n",
    "        for r in range(1, max_r + 1):\n",
    "            fp = false_positive_area(threshold, b, r)\n",
    "            fn = false_negative_area(threshold, b, r)\n",
    "            error = fp * false_positive_weight + fn * false_negative_weight\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                opt = (b, r)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035cbe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose B bands and R rows per band such that B · R = num_perm.\n",
    "B, R = optimal_param(0.717, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5428a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from daft import lit\n",
    "\n",
    "# Band Generation\n",
    "df_bands = (\n",
    "    df_minhash\n",
    "    .with_column(\"bands\", col(\"min_hashes\").list.chunk(R))\n",
    "    .with_column(\"band_idx\", lit(list(range(B))))\n",
    "    .explode(\"bands\", \"band_idx\")\n",
    "    .select(index_col, \"band_idx\", \"bands\")\n",
    ")\n",
    "df_bands.show(20)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping Bands\n",
    "df_grouped = (\n",
    "    df_bands\n",
    "    .groupby(col(\"band_idx\"), col(\"bands\"))\n",
    "    .agg(col(index_col).agg_list().alias(\"nodes\"))\n",
    ")\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04823a",
   "metadata": {},
   "source": [
    "## Connected Components\n",
    "Every band whose `nodes` have more than one entry are now candidates for consideration. \n",
    "Next we are going to leverage a few tricks from graph theory to  @YK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e262bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Graph Edges\n",
    "df_edges = (\n",
    "    df_grouped\n",
    "    .with_column(\"left_edge\", col(\"nodes\").list.min())\n",
    "    .explode(\"nodes\")\n",
    "    .select(\"left_edge\", right_edge=col(\"nodes\"))\n",
    "    .filter(col(\"left_edge\") != col(\"right_edge\"))\n",
    "    .distinct()\n",
    ")\n",
    "df_edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4a7b4d",
   "metadata": {},
   "source": [
    "### 1. Canonicalize edges to undirected form\n",
    "\n",
    "- Remove nulls\n",
    "- remove rows where u == v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e841973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep edges for connected components\n",
    "df_edges_clean = (\n",
    "    df_edges.select(col(\"left_edge\").alias(\"u\"), col(\"right_edge\").alias(\"v\"))\n",
    "    .where(~col(\"u\").is_null())\n",
    "    .where(~col(\"v\").is_null())\n",
    "    .where(col(\"u\") != col(\"v\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbecf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "\n",
    "df_pd_edges = df_edges_clean.select(col(\"u\").cast(daft.DataType.int64()), col(\"v\").cast(daft.DataType.int64())).to_pandas()\n",
    "\n",
    "# using igraph\n",
    "g = ig.Graph.DataFrame(df_pd_edges, directed=False)\n",
    "strong_components = {frozenset(c) for c in g.connected_components(mode=\"strong\")}\n",
    "weak_components = {frozenset(c) for c in g.connected_components(mode=\"weak\")}\n",
    "\n",
    "print(strong_components)\n",
    "print(weak_components)\n",
    "assert strong_components == weak_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ig.plot(\n",
    "    weak_components,\n",
    "    target=ax,\n",
    "    palette=ig.RainbowPalette(),\n",
    "    vertex_size=7,\n",
    "    vertex_color=list(map(int, ig.rescale(components.membership, (0, 200), clamp=True))),\n",
    "    edge_width=0.7\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cd6ec",
   "metadata": {},
   "source": [
    "### Star Contraction with Daft\n",
    "Now we will iteratively compress the graph using two alternating phases until convergence:\n",
    "- Large-star: Every node points to the minimum ID in its neighborhood (including itself). This quickly pulls nodes toward low-ID “hubs.”\n",
    "- Small-star: Re-orient edges to ensure u < v (canonicalize) and repeat contraction, which merges local hubs together.\n",
    "- Repeat large-star then small-star until nothing changes. The “parent” each node ends up pointing to is its component representative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f20a18",
   "metadata": {},
   "source": [
    "### 2. Large-star phase\n",
    "- Group neighbors by u.\n",
    "- Compute min_neighbor = min(neighbors).\n",
    "- Use min(u, min_neighbor) as the node’s “parent.”\n",
    "- Emit edges (u, parent) but only where parent > u to avoid self-loops and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b475ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (\n",
    "    b\n",
    "    # large_star_map\n",
    "    .select(\"u\", \"v\") \n",
    "    .union_all(b.select(col(\"v\").alias(\"u\"), col(\"u\").alias(\"v\"))) # Include upper and lower triangles\n",
    "    .groupby(\"u\").agg_list(\"v\") # Group by u and aggregate the list of v's \n",
    "    .with_column(\"min_edge\", col(\"v\").list.min()) # Find the minimum v for each u and call it min_edge\n",
    "    .with_column(\"min_edge\", (col(\"u\") <= col(\"min_edge\")).if_else(col(\"u\"), col(\"min_edge\"))) # If u is less than the min_edge, use u, otherwise use the min_edge... this is just a sanity check to ensure we are always moving towards lower ids. \n",
    "    .with_column(\"v\", col(\"v\").explode())\n",
    "    .where(col(\"v\") > col(\"u\"))\n",
    "    .where(~col(\"v\").is_null()) #should be a no-op but just in case\n",
    "    .distinct()\n",
    "    .select(col(\"u\"), col(\"v\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea506c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results after 1 large star iteration \n",
    "daft_components = {frozenset([d[\"u\"],*d[\"v\"]]) for d in a.to_pylist()}\n",
    "assert daft_components == strong_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02bb5d4",
   "metadata": {},
   "source": [
    "### 3. Small-star phase\n",
    "- Re-orient all edges so u < v (canonical).\n",
    "- Group neighbors by u, compute min_neighbor, connect (u, parent) like above.\n",
    "- This step merges local minima across previously separate stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: this will fail.\n",
    "def small_star_phase(df: DataFrame):\n",
    "    return (\n",
    "        df\n",
    "        # small_star_map\n",
    "        .select((col(\"u\") > col(\"v\")).if_else(ee(col(\"u\"), col(\"v\")), ee(col(\"v\"), col(\"u\"))).alias(\"e\"))\n",
    "        .select(col(\"e\")[\"*\"])\n",
    "\n",
    "        .groupby(\"u\").agg_list(\"v\")\n",
    "        # small_star_reduce\n",
    "        .with_column(\"min_edge\", col(\"v\").list.min())\n",
    "        .with_column(\"min_edge\", (col(\"u\") <= col(\"min_edge\")).if_else(col(\"u\"), col(\"min_edge\")))\n",
    "        .select(col(\"u\").list.map(ee(daft.element(), col(\"min_edge\"))).alias(\"e\"), col(\"u\"), col(\"min_edge\"))\n",
    "        # TODO: list_append\n",
    "\n",
    "        .explode(\"e\")\n",
    "        .where(~col(\"e\").is_null())\n",
    "        .distinct()\n",
    "        .select(col(\"e\")[\"*\"])\n",
    "        .collect() # Materialize\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758754a",
   "metadata": {},
   "source": [
    "### 4. Convergence check\n",
    "- Compare a stable summary of edges before/after (hash sum is fine).\n",
    "- If stable, stop; otherwise repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence(a: DataFrame, b: DataFrame):\n",
    "    a_hash = a.select(col(\"u\").hash().alias(\"hash\")).sum(\"hash\").to_pydict()[\"hash\"][0]\n",
    "    b_hash = b.select(col(\"u\").hash().alias(\"hash\")).sum(\"hash\").to_pydict()[\"hash\"][0]\n",
    "    if a_hash == b_hash:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed39c63c",
   "metadata": {},
   "source": [
    "### Combining Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7633b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connected_components(\n",
    "    edges: DataFrame,     \n",
    "    left_id_col=\"left_edge\",\n",
    "    right_id_col=\"right_edge\",\n",
    "    output_index_col=index_col,\n",
    "    output_component_col=\"__component__\",\n",
    "):\n",
    "    # Convert column names to u, v\n",
    "    b = (\n",
    "        edges.select(col(left_id_col).alias(\"u\"), col(right_id_col).alias(\"v\"))\n",
    "        .where(~col(\"u\").is_null())\n",
    "        .where(~col(\"v\").is_null())\n",
    "        .collect()\n",
    "    )    \n",
    "    while True:\n",
    "        a = large_star_phase(b)\n",
    "        b = small_star_phase(a)\n",
    "        if check_convergence(a, b):\n",
    "            break\n",
    "    \n",
    "    # Revert column names and return contracted star edges\n",
    "    return (\n",
    "        b\n",
    "        .select(col(\"u\").alias(output_index_col), col(\"v\").alias(output_component_col))\n",
    "        .collect()\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def components(\n",
    "    df: DataFrame,\n",
    "    left_id_col: str = \"u\",\n",
    "    right_id_col: str = \"v\",\n",
    "    output_index_col: str = \"u\",\n",
    "    output_component_col: str = \"component\"\n",
    ") -> DataFrame:\n",
    "    b = (\n",
    "        df.select(col(left_id_col).alias(\"u\"), col(right_id_col).alias(\"v\"))\n",
    "        .where(~col(\"u\").is_null())\n",
    "        .where(~col(\"v\").is_null())\n",
    "        .collect()\n",
    "    )    \n",
    "    while True:\n",
    "        a = (b\n",
    "             # large_star_map\n",
    "             .select(\"u\", \"v\")\n",
    "             .union_all(b.select(col(\"v\").alias(\"u\"), col(\"u\").alias(\"v\")))\n",
    "\n",
    "             .groupby(\"u\").agg_list(\"v\")\n",
    "             # large_star_reduce\n",
    "             .with_column(\"min_edge\", col(\"v\").list.min())\n",
    "             .with_column(\"min_edge\", (col(\"u\") <= col(\"min_edge\")).if_else(col(\"u\"), col(\"min_edge\")))\n",
    "             .select(col(\"u\").list.map(ee(daft.element(), col(\"min_edge\"))).alias(\"e\"), col(\"u\"))\n",
    "\n",
    "             .explode(\"e\")\n",
    "             .where(col(\"e\")[\"v\"] > col(\"u\")).select(\"e\")\n",
    "             .where(~col(\"e\").is_null())\n",
    "             .distinct()\n",
    "             .select(col(\"e\")[\"*\"])\n",
    "             .where(col(\"u\") != col(\"v\"))\n",
    "             .collect()\n",
    "        )\n",
    "        b = (a\n",
    "             # small_star_map\n",
    "             .select((col(\"u\") > col(\"v\")).if_else(ee(col(\"u\"), col(\"v\")), ee(col(\"v\"), col(\"u\"))).alias(\"e\"))\n",
    "             .select(col(\"e\")[\"*\"])\n",
    "\n",
    "             .groupby(\"u\").agg_list(\"v\")\n",
    "             # small_star_reduce\n",
    "             .with_column(\"min_edge\", col(\"v\").list.min())\n",
    "             .with_column(\"min_edge\", (col(\"u\") <= col(\"min_edge\")).if_else(col(\"u\"), col(\"min_edge\")))\n",
    "             .select(col(\"u\").list.map(ee(daft.element(), col(\"min_edge\"))).alias(\"e\"), col(\"u\"), col(\"min_edge\"))\n",
    "             # TODO: list_append\n",
    "\n",
    "             .explode(\"e\")\n",
    "             .where(~col(\"e\").is_null())\n",
    "             .distinct()\n",
    "             .select(col(\"e\")[\"*\"])\n",
    "             .collect()\n",
    "        )\n",
    "        # check convergence\n",
    "        a_hash = a.select(col(\"u\").hash().alias(\"hash\")).sum(\"hash\").to_pydict()[\"hash\"][0]\n",
    "        b_hash = b.select(col(\"u\").hash().alias(\"hash\")).sum(\"hash\").to_pydict()[\"hash\"][0]\n",
    "        if a_hash == b_hash:\n",
    "            return (\n",
    "                b\n",
    "                .select(col(\"u\").alias(output_index_col), col(\"v\").alias(output_component_col))\n",
    "                .collect()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff8ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment = components(\n",
    "    df_edges,\n",
    "    left_id_col=\"left_edge\",\n",
    "    right_id_col=\"right_edge\",\n",
    "    output_index_col=index_col,\n",
    "    output_component_col=\"__component__\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the Star Contraction\n",
    "df_star_edges = connected_components(\n",
    "    edges=df_edges,\n",
    "    left_id_col=\"left_edge\",\n",
    "    right_id_col=\"right_edge\",\n",
    "    output_index_col=index_col,\n",
    "    output_component_col=\"__component__\",\n",
    ")\n",
    "df_star_edges.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a51506",
   "metadata": {},
   "source": [
    "### 5. Final assignment\n",
    "- Treat the final v for each u as the component representative.\n",
    "- Join back to your documents and keep the representative per component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c40f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep one per component (the representative equals the index)\n",
    " (\n",
    "    df\n",
    "    .join(assignment.select(col(index_col), col(\"__component__\")), on=index_col, how=\"left\")\n",
    "    .filter(col(\"__component__\").is_null() | (col(\"__component__\") == col(index_col)))\n",
    "    .exclude(\"__component__\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08557c7c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
