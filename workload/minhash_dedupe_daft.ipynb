{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Daft MinHash Deduplication (Common Crawl)\n",
        "\n",
        "This notebook builds an end-to-end deduplication pipeline over web pages with:\n",
        "- Robust parsing for inconsistent robots/HTTP records\n",
        "- Text extraction and normalization\n",
        "- Tokenization and k-shingle generation\n",
        "- MinHash signatures using Daft expressions\n",
        "- Lightweight LSH banding for candidate duplicate grouping\n",
        "\n",
        "References: Daft Expressions API: [docs.daft.ai Expressions](https://docs.daft.ai/en/stable/api/expressions/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import os\n",
        "from typing import Iterable, List, Optional, Tuple\n",
        "\n",
        "import daft\n",
        "from daft import col, lit\n",
        "from daft.context import get_context\n",
        "\n",
        "# Ensure we have local execution for small samples; scale later\n",
        "get_context().set_runner_local()\n",
        "\n",
        "print(daft.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample input: raw WARC payload-like bytes for demonstration\n",
        "# In practice, read WARC with warcio and project out headers, URLs, and payload\n",
        "sample_records = [\n",
        "    {\n",
        "        \"record_id\": \"rec-1\",\n",
        "        \"raw_bytes\": b\"robots: classic\\r\\nhostname: ip-10-158-89-8.ec2.internal\\r\\nsoftware: Nutch 1.6 (CC)\\r\\nisPartOf: CC-MAIN-2018-17\\r\\noperator: Common Crawl Admin\\r\\ndescription: Wide crawl of the web for April 2018\\r\\npublisher: Common Crawl\\r\\nformat: WARC File Format 1.0\\r\\nconformsTo: http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf\\r\\n\",\n",
        "        \"url\": \"about:robots\"\n",
        "    },\n",
        "    {\n",
        "        \"record_id\": \"rec-2\",\n",
        "        \"raw_bytes\": b\"GET /news-ed-eventi/news-seat HTTP/1.0\\r\\nHost: 00064.dealerseat.com\\r\\nAccept-Encoding: x-gzip, gzip, deflate\\r\\nUser-Agent: CCBot/2.0 (http://commoncrawl.org/faq/)\\r\\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\\r\\nIf-Modified-Since: Fri, 19 Jan 2018 05:39:59 GMT\\r\\n\\r\\n<html><body>SEAT News</body></html>\",\n",
        "        \"url\": \"http://00064.dealerseat.com/news-ed-eventi/news-seat\"\n",
        "    },\n",
        "]\n",
        "\n",
        "df = daft.from_pydict({k: [rec[k] for rec in sample_records] for k in sample_records[0].keys()})\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parser UDF: separate HTTP headers from body; ignore robots-like records\n",
        "import re\n",
        "from html import unescape\n",
        "\n",
        "ROBOTS_HINTS = (b\"robots:\", b\"isPartOf:\", b\"WARC File Format\", b\"Common Crawl\")\n",
        "\n",
        "HEADER_BODY_SPLIT = re.compile(rb\"\\r?\\n\\r?\\n\", re.MULTILINE)\n",
        "TAG_CLEAN = re.compile(r\"<[^>]+>\")\n",
        "WS = re.compile(r\"\\s+\")\n",
        "\n",
        "\n",
        "def parse_payload(raw_bytes: bytes) -> dict:\n",
        "    if raw_bytes is None:\n",
        "        return {\"is_html\": False, \"text\": None}\n",
        "\n",
        "    # Heuristic: drop obvious robots/metadata blocks\n",
        "    lower = raw_bytes[:200].lower()\n",
        "    if any(hint in lower for hint in ROBOTS_HINTS):\n",
        "        return {\"is_html\": False, \"text\": None}\n",
        "\n",
        "    # Split HTTP headers from body if present\n",
        "    parts = HEADER_BODY_SPLIT.split(raw_bytes, maxsplit=1)\n",
        "    body = parts[1] if len(parts) == 2 else parts[0]\n",
        "\n",
        "    # Quick check for html\n",
        "    if b\"<html\" not in body.lower() and b\"<body\" not in body.lower():\n",
        "        # Not clearly HTML; still attempt minimal text extraction\n",
        "        try:\n",
        "            text = body.decode(\"utf-8\", errors=\"ignore\")\n",
        "        except Exception:\n",
        "            return {\"is_html\": False, \"text\": None}\n",
        "        text = unescape(TAG_CLEAN.sub(\" \", text))\n",
        "        text = WS.sub(\" \", text).strip()\n",
        "        return {\"is_html\": False, \"text\": text or None}\n",
        "\n",
        "    try:\n",
        "        html = body.decode(\"utf-8\", errors=\"ignore\")\n",
        "    except Exception:\n",
        "        return {\"is_html\": False, \"text\": None}\n",
        "\n",
        "    text = unescape(TAG_CLEAN.sub(\" \", html))\n",
        "    text = WS.sub(\" \", text).strip()\n",
        "    return {\"is_html\": True, \"text\": text or None}\n",
        "\n",
        "\n",
        "df_parsed = df.with_columns(\n",
        "    {\n",
        "        \"parsed\": col(\"raw_bytes\").udf(parse_payload),\n",
        "        \"record_id\": col(\"record_id\"),\n",
        "        \"url\": col(\"url\"),\n",
        "    }\n",
        ").select([col(\"record_id\"), col(\"url\"), col(\"parsed\")])\n",
        "\n",
        "df_parsed = df_parsed.with_columns(\n",
        "    {\n",
        "        \"is_html\": col(\"parsed\").struct.get(\"is_html\"),\n",
        "        \"text\": col(\"parsed\").struct.get(\"text\"),\n",
        "    }\n",
        ").drop(\"parsed\")\n",
        "\n",
        "df_parsed.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize text, tokenize, and build k-shingles\n",
        "# We'll use Daft string expressions where possible, with small UDFs where needed\n",
        "from typing import List\n",
        "\n",
        "K = 5  # shingle size\n",
        "\n",
        "\n",
        "def tokenize(text: Optional[str]) -> List[str]:\n",
        "    if not text:\n",
        "        return []\n",
        "    # very light tokenization; lowercased, basic split\n",
        "    return [t for t in re.split(r\"[^a-z0-9]+\", text.lower()) if t]\n",
        "\n",
        "\n",
        "def shingles(tokens: List[str], k: int = K) -> List[str]:\n",
        "    if not tokens or k <= 0 or len(tokens) < k:\n",
        "        return []\n",
        "    return [\" \".join(tokens[i : i + k]) for i in range(len(tokens) - k + 1)]\n",
        "\n",
        "\n",
        "df_tokens = (\n",
        "    df_parsed\n",
        "    .with_columns({\"lower_text\": col(\"text\").str.lower()})\n",
        "    .with_columns({\n",
        "        \"tokens\": col(\"lower_text\").udf(tokenize),\n",
        "        \"shingles\": col(\"tokens\").udf(lambda toks: shingles(toks, K)),\n",
        "    })\n",
        "    .drop(\"lower_text\")\n",
        ")\n",
        "\n",
        "df_tokens.select([\"record_id\", \"is_html\", \"url\", \"tokens\", \"shingles\"]).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MinHash signature (UDF fallback). Daft also exposes Expression.minhash; we can swap later.\n",
        "import hashlib\n",
        "from typing import Sequence\n",
        "\n",
        "NUM_PERM = 128\n",
        "ROWS_PER_BAND = 4  # 128/4 = 32 bands\n",
        "\n",
        "\n",
        "def _hash64(b: bytes) -> int:\n",
        "    # Stable 64-bit hash via SHA1 digest\n",
        "    d = hashlib.sha1(b).digest()\n",
        "    return int.from_bytes(d[:8], byteorder=\"big\", signed=False)\n",
        "\n",
        "\n",
        "# Precompute per-permutation salts\n",
        "_SALTS = [f\"mh{seed}\".encode(\"utf-8\") for seed in range(NUM_PERM)]\n",
        "\n",
        "\n",
        "def minhash_signature(shingles: Optional[Sequence[str]], num_perm: int = NUM_PERM) -> List[int]:\n",
        "    if not shingles:\n",
        "        return [2**64 - 1] * num_perm\n",
        "    sig = [2**64 - 1] * num_perm\n",
        "    for sh in shingles:\n",
        "        sh_b = sh.encode(\"utf-8\", errors=\"ignore\")\n",
        "        for i in range(num_perm):\n",
        "            h = _hash64(_SALTS[i] + b\"|\" + sh_b)\n",
        "            if h < sig[i]:\n",
        "                sig[i] = h\n",
        "    return sig\n",
        "\n",
        "\n",
        "def lsh_bands(signature: Sequence[int], rows_per_band: int = ROWS_PER_BAND) -> List[dict]:\n",
        "    if not signature:\n",
        "        return []\n",
        "    n = len(signature)\n",
        "    if rows_per_band <= 0 or n % rows_per_band != 0:\n",
        "        # Trim to nearest multiple\n",
        "        n = (n // rows_per_band) * rows_per_band\n",
        "    bands = []\n",
        "    for b in range(0, n, rows_per_band):\n",
        "        chunk = signature[b : b + rows_per_band]\n",
        "        hasher = hashlib.sha1()\n",
        "        for v in chunk:\n",
        "            hasher.update(v.to_bytes(8, byteorder=\"big\", signed=False))\n",
        "        bucket = int.from_bytes(hasher.digest()[:8], byteorder=\"big\", signed=False)\n",
        "        bands.append({\"band\": b // rows_per_band, \"bucket\": bucket})\n",
        "    return bands\n",
        "\n",
        "\n",
        "df_sig = df_tokens.with_columns({\n",
        "    \"signature\": col(\"shingles\").udf(minhash_signature),\n",
        "    \"num_shingles\": col(\"shingles\").list.length(),\n",
        "})\n",
        "\n",
        "df_sig.select([\"record_id\", \"num_shingles\", \"signature\"]).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSH banding and candidate grouping\n",
        "from daft import functions as F\n",
        "\n",
        "# Produce (band, bucket) pairs\n",
        "bands_df = df_sig.with_columns({\n",
        "    \"bands\": col(\"signature\").udf(lambda sig: lsh_bands(sig, ROWS_PER_BAND))\n",
        "}).explode(\"bands\")\n",
        "\n",
        "bands_df = bands_df.with_columns({\n",
        "    \"band\": col(\"bands\").struct.get(\"band\"),\n",
        "    \"bucket\": col(\"bands\").struct.get(\"bucket\"),\n",
        "}).drop(\"bands\")\n",
        "\n",
        "# Group by (band, bucket) and collect record_ids per bucket\n",
        "bucketed = (\n",
        "    bands_df\n",
        "    .groupby([\"band\", \"bucket\"]) \n",
        "    .agg({\"record_id\": col(\"record_id\").agg_list()})\n",
        "    .with_columns({\"bucket_size\": col(\"record_id\").list.length()})\n",
        ")\n",
        "\n",
        "# Keep only buckets with at least 2 docs\n",
        "candidates = bucketed.where(col(\"bucket_size\") >= lit(2))\n",
        "candidates.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Candidate pairs (within each bucket, make unique unordered pairs)\n",
        "from itertools import combinations\n",
        "\n",
        "def pairs_from_list(ids: List[str]) -> List[Tuple[str, str]]:\n",
        "    if not ids or len(ids) < 2:\n",
        "        return []\n",
        "    # sort for stable, unordered pairs\n",
        "    s = sorted(ids)\n",
        "    return [(a, b) for a, b in combinations(s, 2)]\n",
        "\n",
        "pairs_df = candidates.with_columns({\n",
        "    \"pairs\": col(\"record_id\").udf(pairs_from_list)\n",
        "}).explode(\"pairs\")\n",
        "\n",
        "pairs_df = pairs_df.with_columns({\n",
        "    \"id_a\": col(\"pairs\").list.get(0),\n",
        "    \"id_b\": col(\"pairs\").list.get(1),\n",
        "}).drop(\"pairs\")\n",
        "\n",
        "pairs_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
