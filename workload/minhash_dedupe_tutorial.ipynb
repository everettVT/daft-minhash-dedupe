{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial: Deduplicate Web Text with MinHash + LSH (Guided Lesson)\n",
        "\n",
        "By the end of this lesson you will run a small, reliable pipeline that:\n",
        "- extracts text blocks from HTML,\n",
        "- normalizes text,\n",
        "- computes MinHash signatures and LSH buckets,\n",
        "- finds connected components to group near-duplicates,\n",
        "- exports a deduplicated dataset and a duplicates sample.\n",
        "\n",
        "We will take one safe path with clear actions, expected results, and checkpoints. No AWS required by default.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Prerequisites and Preflight\n",
        "\n",
        "Action:\n",
        "- Ensure Python 3.11+ and install pinned packages.\n",
        "- Run the preflight cell to verify imports and versions.\n",
        "\n",
        "Expected result:\n",
        "- A green check and versions printed.\n",
        "\n",
        "If it fails:\n",
        "- Re-run install; ensure your virtual environment is active.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "python - << 'PY'\n",
        "import sys\n",
        "print(sys.version)\n",
        "PY\n",
        "\n",
        "python -m pip install --quiet 'daft[aws,pandas]==0.3.9' selectolax==0.3.23 python-dotenv==1.0.1 scipy==1.12.0 matplotlib==3.8.4 igraph==0.11.6 numpy==1.26.4 pandas==2.2.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preflight: versions and imports\n",
        "from __future__ import annotations\n",
        "import importlib, sys\n",
        "mods = {\n",
        "    'daft': '0.3.9',\n",
        "    'selectolax': '0.3.23',\n",
        "    'scipy': '1.12.0',\n",
        "    'matplotlib': '3.8.4',\n",
        "    'igraph': '0.11.6',\n",
        "    'numpy': '1.26.4',\n",
        "    'pandas': '2.2.2',\n",
        "}\n",
        "\n",
        "ok = True\n",
        "for name, expected in mods.items():\n",
        "    m = importlib.import_module(name)\n",
        "    v = getattr(m, '__version__', None)\n",
        "    print(f\"{name}=={v}\")\n",
        "    if v and expected and v != expected:\n",
        "        print(f\"[warn] expected {expected} but got {v} for {name}\")\n",
        "\n",
        "if ok:\n",
        "    print(\"✅ Preflight OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load sample HTML data (no AWS)\n",
        "\n",
        "Action:\n",
        "- Load a tiny, in-memory HTML sample into a table with `WARC-Record-ID`, `warc_content`, and `WARC-Identified-Payload-Type`.\n",
        "\n",
        "Expected result:\n",
        "- Table has > 0 rows and the payload type is `text/html`.\n",
        "\n",
        "Checkpoint:\n",
        "- A small preview prints with 3–5 rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import daft\n",
        "from daft import col\n",
        "\n",
        "HTML_DOCS = [\n",
        "    (\"warc:1\", b\"HTTP/1.1 200 OK\\r\\n\\r\\n<html><head><title>Example 1</title></head><body><h1>Alpha</h1><p>Hello world world!</p></body></html>\", \"text/html\"),\n",
        "    (\"warc:2\", b\"HTTP/1.1 200 OK\\r\\n\\r\\n<html><head><title>Example 2</title></head><body><h1>Alpha</h1><p>Hello world!</p></body></html>\", \"text/html\"),\n",
        "    (\"warc:3\", b\"HTTP/1.1 200 OK\\r\\n\\r\\n<html><head><title>Other</title></head><body><h2>Bravo</h2><p>Different content altogether.</p></body></html>\", \"text/html\"),\n",
        "]\n",
        "\n",
        "df_warc = daft.from_pydict({\n",
        "    \"WARC-Record-ID\": [x[0] for x in HTML_DOCS],\n",
        "    \"warc_content\":   [x[1] for x in HTML_DOCS],\n",
        "    \"WARC-Identified-Payload-Type\": [x[2] for x in HTML_DOCS],\n",
        "}).collect()\n",
        "\n",
        "assert df_warc.count_rows() > 0, \"No rows loaded\"\n",
        "df_warc.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Extract text blocks from HTML\n",
        "\n",
        "Action:\n",
        "- Strip HTTP headers and extract visible text blocks.\n",
        "\n",
        "Expected result:\n",
        "- A table of text blocks with `block_id` and `block` columns, > 0 rows.\n",
        "\n",
        "Checkpoint:\n",
        "- Show 3 sample blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from selectolax.parser import HTMLParser\n",
        "\n",
        "index_col = \"block_id\"\n",
        "content_col = \"block\"\n",
        "\n",
        "@daft.func()\n",
        "def remove_http_headers(x: bytes) -> str:\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    s = x.decode(\"utf-8\", errors=\"ignore\")\n",
        "    parts = s.split(\"\\r\\n\\r\\n\")\n",
        "    if len(parts) > 1:\n",
        "        return parts[1]\n",
        "    parts = s.split(\"\\n\\n\")\n",
        "    return parts[1] if len(parts) > 1 else s\n",
        "\n",
        "@daft.func()\n",
        "def extract_blocks(html: str) -> list[str]:\n",
        "    tree = HTMLParser(html)\n",
        "    for n in tree.css(\"script,style,noscript\"):\n",
        "        n.decompose()\n",
        "    blocks = []\n",
        "    for node in tree.css(\"title, article, main, p, h1, h2, h3, h4, h5, h6, li, div, section\"):\n",
        "        txt = node.text(separator=\" \", strip=True)\n",
        "        if txt and len(txt) >= 20:\n",
        "            blocks.append(txt)\n",
        "    return blocks\n",
        "\n",
        "@daft.func()\n",
        "def get_block_idx(blocks: list[str]) -> list[int]:\n",
        "    return list(range(len(blocks)))\n",
        "\n",
        "df_html = (\n",
        "    df_warc\n",
        "    .where(col(\"WARC-Identified-Payload-Type\") == \"text/html\")\n",
        "    .with_column(\"content_raw\", remove_http_headers(col(\"warc_content\")))\n",
        "    .where(col(\"content_raw\") != \"\")\n",
        ")\n",
        "\n",
        "df_text = (\n",
        "    df_html\n",
        "    .with_column(\"blocks\", extract_blocks(col(\"content_raw\")))\n",
        "    .with_column(\"block_idx\", get_block_idx(col(\"blocks\")))\n",
        "    .explode(\"blocks\", \"block_idx\")\n",
        "    .where(col(\"blocks\") != \"\")\n",
        "    .with_column(index_col, col(\"WARC-Record-ID\") + \"-\" + col(\"block_idx\"))\n",
        "    .with_column(content_col, col(\"blocks\"))\n",
        "    .select(\"WARC-Record-ID\", index_col, content_col)\n",
        ").collect()\n",
        "\n",
        "assert df_text.count_rows() > 0, \"No blocks extracted\"\n",
        "df_text.show(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Normalize text\n",
        "\n",
        "Action:\n",
        "- Normalize punctuation, case, Unicode, and whitespace into `content_normalized`.\n",
        "\n",
        "Expected result:\n",
        "- New column present with lowercased, de‑noised text.\n",
        "\n",
        "Checkpoint:\n",
        "- Show 3 rows comparing `block` vs `content_normalized`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_norm = df_text.with_column(\n",
        "    \"content_normalized\",\n",
        "    col(content_col).str.normalize(\n",
        "        remove_punct=True,\n",
        "        lowercase=True,\n",
        "        nfd_unicode=True,\n",
        "        white_space=True,\n",
        "    ),\n",
        ").collect()\n",
        "\n",
        "assert \"content_normalized\" in df_norm.column_names(), \"Normalization failed\"\n",
        "df_norm.select(index_col, content_col, \"content_normalized\").show(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: MinHash signatures\n",
        "\n",
        "Action:\n",
        "- Compute MinHash vectors with fixed parameters.\n",
        "\n",
        "Expected result:\n",
        "- A `min_hashes` column of length `K` per row.\n",
        "\n",
        "Checkpoint:\n",
        "- Show 3 rows with `min_hashes[:8]` preview.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "K = 64\n",
        "SEED = 42\n",
        "NGRAM_SIZE = 5\n",
        "\n",
        "df_minhash = (\n",
        "    df_norm\n",
        "    .with_column(\n",
        "        \"min_hashes\",\n",
        "        col(\"content_normalized\").minhash(\n",
        "            num_hashes=K,\n",
        "            ngram_size=NGRAM_SIZE,\n",
        "            seed=SEED,\n",
        "            hash_function=\"xxhash\",\n",
        "        ),\n",
        "    )\n",
        ").collect()\n",
        "\n",
        "# Quick preview of the first 8 hashes\n",
        "import json\n",
        "rows = df_minhash.select(index_col, \"min_hashes\").limit(3).to_pydict()\n",
        "for i in range(len(rows[index_col])):\n",
        "    print(rows[index_col][i], rows[\"min_hashes\"][i][:8])\n",
        "\n",
        "assert df_minhash.count_rows() > 0, \"MinHash failed\"\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
